{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Note for basic concepts for tensorflow**\n",
    "---\n",
    "<br>"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the basic packages\n",
    "- Can set tensorflow to eager mode to run faster on small functions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try use tensors\n",
    "- tf.zeros: all 0\n",
    "- tf.ones: all 1\n",
    "- tf.fill: all target number\n",
    "- tf.constant(\\[element_list\\])\n",
    "- tf.mutmul(a, b): multiplication\n",
    "- Use Variable to create tensor with target distribution, set a seed and the w will remain the same every time fire up the programe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.,3.]])\n",
    "b = tf.constant([[4.],[2.]])\n",
    "# tensor multiplication, need to check if the two tensors can be multiplied.\n",
    "y = tf.matmul(a, b) \n",
    "# excute the computation using tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y)\n",
    "    print(y)\n",
    "# use Variable to create tensor\n",
    "w = tf.Variable(tf.random_normal([2, 3], stddev=2, mean=0, seed=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the gloable variable\n",
    "- define the input data type, shape (dimensions, None means any natural number)\n",
    "- feed data into model, the data shape should be compatiable with the pre-defined one."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    in_data = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    sess.run(y, feed_dict={in_data:[[.5, .6],[.4, .4],[.6, .9]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hperparameter settings\n",
    "- loss function: MSE 均方误差, CrossEntropyLoss 等\n",
    "- optimizer function: GSD, Momemtum, Adam, RSMprop 等\n",
    "- activation functions: sigmoid, tanh \\[Linear\\] / ReLU \\[Non-linear\\]\n",
    "- batch training: use batch size to define how many samples would be considered during one step of optimization.\n",
    "- we also use sigmoid and softmax for two / multi classes classification tasks to turn the predictions into probabilities.\n",
    "- learning rate: the decay learning rate: actually we want the lr to be adjust to a figure that can make the function converge quickly and can also jump out of local minimals.\n",
    "- Momemtm, 动量，滑动平均值"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = None # the real class of the sample\n",
    "predict = None # the answer of the classifer / module\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.square(predict-label))\n",
    "loss_cross = - tf.reduce_mean(label * tf.log(predict))\n",
    "# optimizer\n",
    "optim = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "# softmax\n",
    "cross_with_softmax = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predict, labels=tf.arg_max(label, 1))\n",
    "c_w_s_mean = tf.reduce_mean(cross_with_softmax)\n",
    "# the decay learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate_decay = tf.train.exponential_decay(\n",
    "    learning_rate=0.01, # base learning rate\n",
    "    global_step=global_step, # count how many iterations of batch train is conducted, start from 0.\n",
    "    decay_steps=5, # the learning rate decay would happen after how many times of batch training\n",
    "    decay_rate=0.9, # the speed of decay \n",
    "    staircase=True\n",
    ")\n",
    "# Momemtm\n",
    "ema = tf.train.ExponentialMovingAverage(\n",
    "    decay=0.99,\n",
    "    num_updates=global_step # the current iteration time\n",
    ")\n",
    "# apply method to all the changable parameters\n",
    "ema_op = ema.apply(tf.trainable_variables())\n",
    "with tf.control_dependencies(control_inputs=[train_Step, ema_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "# check target parameter & run the ema_op\n",
    "ema.average(var=para_name)\n",
    "tf.Session.run(ema_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularzation in tensorflow\n",
    "- l1 norm: tf.contrib.layers.l1_regularizer()\n",
    "- l2 nrom: tf.contrib.layers.l2_regularizer()\n",
    "- collect all the losses to a certain list\n",
    "- add the collected list data to a exist list"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.layers.l1_regularizer()\n",
    "tf.contrib.layers.l2_regularizer()\n",
    "# collect all the losses\n",
    "tf.add_to_collection('loss_list', tf.contrib.layers.l1_regularizer(regularizer)(parameter))\n",
    "loss_list = loss + tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use a dataset: MNIST as an example\n",
    "- this dataset is intergrated in tensorflow lib: train as training set and t10k as test set"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./dataset/Minst', one_hot=True) # use one-hot coding to encode the mnist dataset\n",
    "mnist.train.num_examples # the size of the training set\n",
    "mnist.validation.num_examples \n",
    "mnist.test.num_examples \n",
    "mnist.train.labels[0]\n",
    "mnist.train.images[0]\n",
    "batch_sample, batch_label = mnist.train.next_batch(BATCH_SIZE) # ectract a batch of data from mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some normal operations on data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tf.get_collection(\"collection_name\") # get all the variables out as a list from a collection\n",
    "tf.add_n([]) # add a list in correspond sptial position\n",
    "tf.cast(sample, dtype) # transfer data type\n",
    "tf.arg_max(sample, axis) # find the index of the max value at certain dimension\n",
    "os.path.join(\"home\", \"name\") # return home/name\n",
    "moudle_saver = tf.train.Saver()\n",
    "moudle_saver.save(tf.Session(), os.path.join(\"save_path\", \"moudle_name\"), global_step=global_step) # save the current moudle to a target place\n",
    "check_point = tf.train.get_checkpoint_state(\"path\")\n",
    "if check_point and check_point.model_checkpoint_path:\n",
    "    moudle_saver.restore(tf.Session(), check_point.model_checkpoint_path) # restore the moudle back to a session\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "ema_restore = ema.variables_to_restore() # restore the parameters for exponetial moving average\n",
    "ema_saver = tf.train.Saver(ema_restore)\n",
    "# caculate the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(label, 1), tf.argmax(predict, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple training example\n",
    "### Main functions\n",
    "- forward(train_sample, regularizer): 前向传播，定义网络框架\n",
    "- getWeight(shape, regularizer): 初始化权重，正则化权重\n",
    "- getBias(shape): 根据每层中神经元个数初始化偏置\n",
    "- backward(): 反向传播，在每个/批epoch中执行，通过loss和梯度优化参数\n",
    "### Other key components\n",
    "- regularization: 正则标准，是否使用\n",
    "- learning rate: 是否decay\n",
    "- optimizer: 配置优化器及其参数\n",
    "- ema: 滑动平均，对参数优化时，考虑过去时间点上发生变化的梯度\n",
    "- global_step: trainable=False，用于统计全局迭代次数情况\n",
    "- with tf.Session() as sess: 进入训练循环，利用sess执行命令"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages and libs, then set tensorflow to be running in the eager mode\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# define the hyperparameters and random generate the dataset\n",
    "BATCH_SIZE = 30\n",
    "SEED = 2 # fix the generate result (dataset)\n",
    "EPOCH = 40000 # how many iterations we want\n",
    "\n",
    "# prepare the dataset\n",
    "rand = np.random.RandomState(SEED)\n",
    "training_data = rand.randn(300, 2) # 300 sample matrix with 2 dimensions\n",
    "training_label = [int((training_data_0 * training_data_0 + training_data_1 * training_data_1) < 2 for (training_data_0, training_data_1) in training_data)]\n",
    "training_label_content = [['red' if label else 'blue'] for label in training_label]\n",
    "# reshpe the training dataset\n",
    "training_data = np.vstack(training_data).reshape(-1, 2)\n",
    "training_label = np.vstack(training_label).reshape(-1, 1)\n",
    "# use the training_label_content to plot the generated training samples\n",
    "plt.scatter(training_data[:,0], training_data[:,1], c=np.squeeze(training_label_content))\n",
    "plt.show()\n",
    "\n",
    "# define the network\n",
    "def getWeight(shape, regularizer):\n",
    "    # randomly init the weight from a normal distribution\n",
    "    weight = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    # define the regularization\n",
    "    tf.add_to_collection('loss_list', tf.contrib.layers.l2_regularizer(regularizer)(weight))\n",
    "    return weight\n",
    "\n",
    "def getBias(shape):\n",
    "    # init the bias for the network neruals\n",
    "    bias = tf.Variable(tf.constant(0.01, shape=shape))\n",
    "    return bias\n",
    "\n",
    "# define the shape of the input variables\n",
    "train_sample = tf.placeholder(tf.float32, shape=(None, 2)) # matrix of 2 dimensions with no number limitataion\n",
    "train_label = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# forward broadcasting through the first & second linear layer\n",
    "weight_1 = getWeight([2, 11], 0.01)\n",
    "bias_1 = getBias([11])\n",
    "predict_1 = tf.nn.relu(tf.matmul(train_sample, weight_1) + bias_1)\n",
    "weight_2 = getWeight([11, 1], 0.01)\n",
    "bias_2 = getBias([1])\n",
    "predict_2 = tf.matmul(predict_1, weight_2) + bias_2\n",
    "\n",
    "# define loss function and our optimizer\n",
    "loss_mse = tf.reduce_mean(tf.square(label - predict_2))\n",
    "loss_list = loss_mse + tf.add_n(tf.get_collection('losslist'))\n",
    "train_optim = tf.train.AdamOptimizer(0.0001).minimize(loss=loss_mse) # not using the l2 regularziation\n",
    "train_optim_reg = tf.train.AdamOptimizer(0.0001).minimize(loss=loss_list)\n",
    "\n",
    "# excute our iterations, this time we ignore the l2 regularization\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(EPOCH):\n",
    "        # every epoch we grab a batch of sample data and their labels, put into our optimizer\n",
    "        start = (epoch + BATCH_SIZE) % 300\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_optim, feed_dict={train_sample:training_data[start: end], train_label:training_label[start: end]})\n",
    "        if epoch % 2000 == 0:\n",
    "            loss_mse_v = sess.run(loss_mse, feed_dict={train_sample:training_data, train_label:training_label})\n",
    "            print(\"At epoch: \" + epoch + \", losss is: \" + loss_mse_v)\n",
    "    # 在(-3,3)区间步长0.01生成二位网格区间，并且拉直构成二列矩阵，本质为坐标点集合\n",
    "    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # show the final moudle parameters\n",
    "    predict = sess.run(train_label, feed_dict={train_sample:grid})\n",
    "    predict = predict.reshape(xx.shape)\n",
    "    print(\"Weight of 1 layer:\" + sess.run(weight_1))\n",
    "    print(\"Bias of 1 layer:\" + sess.run(bias_1))\n",
    "    print(\"Weight of 2 layer:\" + sess.run(weight_2))\n",
    "    print(\"Bias of 2 layer:\" + sess.run(bias_2))\n",
    "\n",
    "plt.scatter(training_data[:,0], training_data[:,1], c=np.squeeze(training_label_content))\n",
    "plt.contour(xx, yy, probs, levels=[.5])\n",
    "plt.show()\n",
    "\n",
    "# excute another network, this time we use the l2 regularization\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(EPOCH):\n",
    "        # every epoch we grab a batch of sample data and their labels, put into our optimizer\n",
    "        start = (epoch + BATCH_SIZE) % 300\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_optim_reg, feed_dict={train_sample:training_data[start: end], train_label:training_label[start: end]})\n",
    "        if epoch % 2000 == 0:\n",
    "            loss_mse_v = sess.run(loss_mse, feed_dict={train_sample:training_data, train_label:training_label})\n",
    "            print(\"At epoch: \" + epoch + \", losss is: \" + loss_mse_v)\n",
    "    # 在(-3,3)区间步长0.01生成二位网格区间，并且拉直构成二列矩阵，本质为坐标点集合\n",
    "    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # show the final moudle parameters\n",
    "    predict = sess.run(train_label, feed_dict={train_sample:grid})\n",
    "    predict = predict.reshape(xx.shape)\n",
    "    print(\"Weight of 1 layer:\" + sess.run(weight_1))\n",
    "    print(\"Bias of 1 layer:\" + sess.run(bias_1))\n",
    "    print(\"Weight of 2 layer:\" + sess.run(weight_2))\n",
    "    print(\"Bias of 2 layer:\" + sess.run(bias_2))\n",
    "\n",
    "plt.scatter(training_data[:,0], training_data[:,1], c=np.squeeze(training_label_content))\n",
    "plt.contour(xx, yy, probs, levels=[.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}