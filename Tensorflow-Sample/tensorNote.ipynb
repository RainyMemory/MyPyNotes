{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Note for basic concepts for tensorflow**\n",
    "---\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the basic packages\n",
    "- Can set tensorflow to eager mode to run faster on small functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try use tensors\n",
    "- tf.zeros: all 0\n",
    "- tf.ones: all 1\n",
    "- tf.fill: all target number\n",
    "- tf.constant(\\[element_list\\])\n",
    "- tf.mutmul(a, b): multiplication\n",
    "- Use Variable to create tensor with target distribution, set a seed and the w will remain the same every time fire up the programe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1.,3.]])\n",
    "b = tf.constant([[4.],[2.]])\n",
    "# tensor multiplication, need to check if the two tensors can be multiplied.\n",
    "y = tf.matmul(a, b) \n",
    "# excute the computation using tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y)\n",
    "    print(y)\n",
    "# use Variable to create tensor\n",
    "w = tf.Variable(tf.random_normal([2, 3], stddev=2, mean=0, seed=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Init the global variable\n",
    "- define the input data type, shape (dimensions, None means any natural number)\n",
    "- feed data into model, the data shape should be compatiable with the pre-defined one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    in_data = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "    sess.run(y, feed_dict={in_data:[[.5, .6],[.4, .4],[.6, .9]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hperparameter settings\n",
    "- loss function: MSE 均方误差, CrossEntropyLoss 等\n",
    "- optimizer function: GSD, Momemtum, Adam, RSMprop 等\n",
    "- activation functions: sigmoid, tanh \\[Linear\\] / ReLU \\[Non-linear\\]\n",
    "- batch training: use batch size to define how many samples would be considered during one step of optimization.\n",
    "- we also use sigmoid and softmax for two / multi classes classification tasks to turn the predictions into probabilities.\n",
    "- learning rate: the decay learning rate: actually we want the lr to be adjust to a figure that can make the function converge quickly and can also jump out of local minimals.\n",
    "- Momemtm, 动量，滑动平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = None # the real class of the sample\n",
    "predict = None # the answer of the classifer / module\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.square(predict-label))\n",
    "loss_cross = - tf.reduce_mean(label * tf.log(predict))\n",
    "# optimizer\n",
    "optim = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "# softmax\n",
    "cross_with_softmax = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predict, labels=tf.arg_max(label, 1))\n",
    "c_w_s_mean = tf.reduce_mean(cross_with_softmax)\n",
    "# the decay learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate_decay = tf.train.exponential_decay(\n",
    "    learning_rate=0.01, # base learning rate\n",
    "    global_step=global_step, # count how many iterations of batch train is conducted, start from 0.\n",
    "    decay_steps=5, # the learning rate decay would happen after how many times of batch training\n",
    "    decay_rate=0.9, # the speed of decay \n",
    "    staircase=True\n",
    ")\n",
    "# Momemtm\n",
    "ema = tf.train.ExponentialMovingAverage(\n",
    "    decay=0.99,\n",
    "    num_updates=global_step # the current iteration time\n",
    ")\n",
    "# apply method to all the changable parameters\n",
    "ema_op = ema.apply(tf.trainable_variables())\n",
    "with tf.control_dependencies(control_inputs=[train_Step, ema_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "# check target parameter & run the ema_op\n",
    "ema.average(var=para_name)\n",
    "tf.Session.run(ema_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in tensorflow\n",
    "- l1 norm: tf.contrib.layers.l1_regularizer()\n",
    "- l2 nrom: tf.contrib.layers.l2_regularizer()\n",
    "- collect all the losses to a certain list\n",
    "- add the collected list data to a exist list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.contrib.layers.l1_regularizer()\n",
    "tf.contrib.layers.l2_regularizer()\n",
    "# collect all the losses\n",
    "tf.add_to_collection('loss_list', tf.contrib.layers.l1_regularizer(regularizer)(parameter))\n",
    "loss_list = loss + tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use a dataset: MNIST as an example\n",
    "- this dataset is intergrated in tensorflow lib: train as training set and t10k as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../dataset/Mnist/MNIST/raw', one_hot=True) # use one-hot coding to encode the mnist dataset\n",
    "mnist.train.num_examples # the size of the training set\n",
    "mnist.validation.num_examples \n",
    "mnist.test.num_examples \n",
    "mnist.train.labels[0]\n",
    "mnist.train.images[0]\n",
    "batch_sample, batch_label = mnist.train.next_batch(BATCH_SIZE) # ectract a batch of data from mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some normal operations on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tf.get_collection(\"collection_name\") # get all the variables out as a list from a collection\n",
    "tf.add_n([]) # add a list in correspond sptial position\n",
    "tf.cast(sample, dtype) # transfer data type\n",
    "tf.arg_max(sample, axis) # find the index of the max value at certain dimension\n",
    "os.path.join(\"home\", \"name\") # return home/name\n",
    "moudle_saver = tf.train.Saver()\n",
    "moudle_saver.save(tf.Session(), os.path.join(\"save_path\", \"moudle_name\"), global_step=global_step) # save the current moudle to a target place\n",
    "check_point = tf.train.get_checkpoint_state(\"path\")\n",
    "if check_point and check_point.model_checkpoint_path:\n",
    "    moudle_saver.restore(tf.Session(), check_point.model_checkpoint_path) # restore the moudle back to a session\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "ema_restore = ema.variables_to_restore() # restore the parameters for exponetial moving average\n",
    "ema_saver = tf.train.Saver(ema_restore)\n",
    "# caculate the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(label, 1), tf.argmax(predict, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A simple training example\n",
    "### Main functions\n",
    "- forward(train_sample, regularizer): 前向传播，定义网络框架\n",
    "- getWeight(shape, regularizer): 初始化权重，正则化权重\n",
    "- getBias(shape): 根据每层中神经元个数初始化偏置\n",
    "- backward(): 反向传播，在每个/批epoch中执行，通过loss和梯度优化参数\n",
    "### Other key components\n",
    "- regularization: 正则标准，是否使用\n",
    "- learning rate: 是否decay\n",
    "- optimizer: 配置优化器及其参数\n",
    "- ema: 滑动平均，对参数优化时，考虑过去时间点上发生变化的梯度\n",
    "- global_step: trainable=False，用于统计全局迭代次数情况\n",
    "- with tf.Session() as sess: 进入训练循环，利用sess执行命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages and libs, then set tensorflow to be running in the eager mode\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# define the hyperparameters and random generate the dataset\n",
    "BATCH_SIZE = 30\n",
    "SEED = 2 # fix the generate result (dataset)\n",
    "EPOCH = 40000 # how many iterations we want\n",
    "\n",
    "# prepare the dataset\n",
    "rand = np.random.RandomState(SEED)\n",
    "training_data = rand.randn(300, 2) # 300 sample matrix with 2 dimensions\n",
    "training_label = [int((training_data_0 * training_data_0 + training_data_1 * training_data_1) < 2 for (training_data_0, training_data_1) in training_data)]\n",
    "training_label_content = [['red' if label else 'blue'] for label in training_label]\n",
    "# reshpe the training dataset\n",
    "training_data = np.vstack(training_data).reshape(-1, 2)\n",
    "training_label = np.vstack(training_label).reshape(-1, 1)\n",
    "# use the training_label_content to plot the generated training samples\n",
    "plt.scatter(training_data[:,0], training_data[:,1], c=np.squeeze(training_label_content))\n",
    "plt.show()\n",
    "\n",
    "# define the network\n",
    "def getWeight(shape, regularizer):\n",
    "    # randomly init the weight from a normal distribution\n",
    "    weight = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    # define the regularization\n",
    "    tf.add_to_collection('loss_list', tf.contrib.layers.l2_regularizer(regularizer)(weight))\n",
    "    return weight\n",
    "\n",
    "def getBias(shape):\n",
    "    # init the bias for the network neruals\n",
    "    bias = tf.Variable(tf.constant(0.01, shape=shape))\n",
    "    return bias\n",
    "\n",
    "# define the shape of the input variables\n",
    "train_sample = tf.placeholder(tf.float32, shape=(None, 2)) # matrix of 2 dimensions with no number limitataion\n",
    "train_label = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# forward broadcasting through the first & second linear layer\n",
    "weight_1 = getWeight([2, 11], 0.01)\n",
    "bias_1 = getBias([11])\n",
    "predict_1 = tf.nn.relu(tf.matmul(train_sample, weight_1) + bias_1)\n",
    "weight_2 = getWeight([11, 1], 0.01)\n",
    "bias_2 = getBias([1])\n",
    "predict_2 = tf.matmul(predict_1, weight_2) + bias_2\n",
    "\n",
    "# define loss function and our optimizer\n",
    "loss_mse = tf.reduce_mean(tf.square(label - predict_2))\n",
    "loss_list = loss_mse + tf.add_n(tf.get_collection('losslist'))\n",
    "train_optim = tf.train.AdamOptimizer(0.0001).minimize(loss=loss_mse) # not using the l2 regularziation\n",
    "train_optim_reg = tf.train.AdamOptimizer(0.0001).minimize(loss=loss_list)\n",
    "\n",
    "# excute our iterations, this time we ignore the l2 regularization\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(EPOCH):\n",
    "        # every epoch we grab a batch of sample data and their labels, put into our optimizer\n",
    "        start = (epoch + BATCH_SIZE) % 300\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_optim, feed_dict={train_sample:training_data[start: end], train_label:training_label[start: end]})\n",
    "        if epoch % 2000 == 0:\n",
    "            loss_mse_v = sess.run(loss_mse, feed_dict={train_sample:training_data, train_label:training_label})\n",
    "            print(\"At epoch: \" + epoch + \", losss is: \" + loss_mse_v)\n",
    "    # 在(-3,3)区间步长0.01生成二位网格区间，并且拉直构成二列矩阵，本质为坐标点集合\n",
    "    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # show the final moudle parameters\n",
    "    predict = sess.run(train_label, feed_dict={train_sample:grid})\n",
    "    predict = predict.reshape(xx.shape)\n",
    "    print(\"Weight of 1 layer:\" + sess.run(weight_1))\n",
    "    print(\"Bias of 1 layer:\" + sess.run(bias_1))\n",
    "    print(\"Weight of 2 layer:\" + sess.run(weight_2))\n",
    "    print(\"Bias of 2 layer:\" + sess.run(bias_2))\n",
    "\n",
    "plt.scatter(training_data[:,0], training_data[:,1], c=np.squeeze(training_label_content))\n",
    "plt.contour(xx, yy, probs, levels=[.5])\n",
    "plt.show()\n",
    "\n",
    "# excute another network, this time we use the l2 regularization\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(EPOCH):\n",
    "        # every epoch we grab a batch of sample data and their labels, put into our optimizer\n",
    "        start = (epoch + BATCH_SIZE) % 300\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_optim_reg, feed_dict={train_sample:training_data[start: end], train_label:training_label[start: end]})\n",
    "        if epoch % 2000 == 0:\n",
    "            loss_mse_v = sess.run(loss_mse, feed_dict={train_sample:training_data, train_label:training_label})\n",
    "            print(\"At epoch: \" + epoch + \", losss is: \" + loss_mse_v)\n",
    "    # 在(-3,3)区间步长0.01生成二位网格区间，并且拉直构成二列矩阵，本质为坐标点集合\n",
    "    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # show the final moudle parameters\n",
    "    predict = sess.run(train_label, feed_dict={train_sample:grid})\n",
    "    predict = predict.reshape(xx.shape)\n",
    "    print(\"Weight of 1 layer:\" + sess.run(weight_1))\n",
    "    print(\"Bias of 1 layer:\" + sess.run(bias_1))\n",
    "    print(\"Weight of 2 layer:\" + sess.run(weight_2))\n",
    "    print(\"Bias of 2 layer:\" + sess.run(bias_2))\n",
    "\n",
    "plt.scatter(training_data[:,0], training_data[:,1], c=np.squeeze(training_label_content))\n",
    "plt.contour(xx, yy, probs, levels=[.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample: Build a CNN with tensorflow\n",
    "### Main steps:\n",
    "- forward: 向前传播，构建参数及网络框架，执行预测\n",
    "- backward: 向后传播，对结果进行验证评估，执行参数优化\n",
    "- excute the training process and check the result\n",
    "- test: 测试模型效果，注意关闭train (train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward\n",
    "import tensorflow as tf \n",
    "\n",
    "# setting hyper parameters\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "CONV1_SIZE = 5\n",
    "CONV1_LAYER = 32\n",
    "CONV2_SIZE = 5\n",
    "CONV2_LAYER = 64\n",
    "FC_LAYER = 512\n",
    "\n",
    "# handle the weight parameters\n",
    "def getWeight(shape, regularizer):\n",
    "    weight = tf.Variable(tf.truncated_normal(shape, stddev=.1))\n",
    "    if regularizer != None: \n",
    "        tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(weight))\n",
    "        return weight\n",
    "\n",
    "# handle the neurals bias\n",
    "def getBias(shape):\n",
    "    bias = tf.Variable(tf.zeros(shape)) # all 0 bias\n",
    "    return bias\n",
    "\n",
    "# define the conv and maxpool layer\n",
    "def conv2d(input_x, weight):\n",
    "    return tf.nn.conv2d(input_x, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxPool2x2(input_x):\n",
    "    return tf.nn.max_pool(input_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def forward(input_x, train, regularizer):\n",
    "    # define the first conv-active-pooling process\n",
    "    conv1_w = getWeight([CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_LAYER], regularizer)\n",
    "    conv1_b = getBias([CONV1_LAYER])\n",
    "    conv1 = conv2d(input_x, conv1_w)\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b))\n",
    "    pool1 = maxPool2x2(relu1)\n",
    "\n",
    "    # define the second one\n",
    "    conv2_w = getWeight([CONV2_SIZE, CONV2_SIZE, CONV1_LAYER, CONV2_LAYER], regularizer)\n",
    "    conv2_b = getBias([CONV2_LAYER])\n",
    "    conv2 = conv2d(pool1, conv2_w)\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))\n",
    "    pool2 = maxPool2x2(relu2)\n",
    "\n",
    "    # reshape the output into only one dimension for the coming fc layer\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "    reshape_x = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "    # define the fc layers, and if we are under training, do the drop out after each fc layer\n",
    "    fc1_w = getWeight([nodes, FC_LAYER], regularizer)\n",
    "    fc1_b = getBias([FC_LAYER])\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshape_x, fc1_w) + fc1_b)\n",
    "    if train:\n",
    "        fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    fc2_w = getWeight([FC_LAYER, NUM_CLASSES], regularizer)\n",
    "    fc2_b = getBias([NUM_CLASSES])\n",
    "    out = tf.matmul(fc1, fc2_w) + fc2_b\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward\n",
    "import tensorflow as tf \n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "# define the hyperparameters\n",
    "BATCH_SIZE = 100\n",
    "LEARN_RATE_BASE = .005\n",
    "LEARN_RATE_DECAY = .99\n",
    "REGULARIZER = .0001\n",
    "EPOCH = 50000\n",
    "MOV_AVG_DECAY = .99\n",
    "MOU_PATH = \"./moudle/\"\n",
    "MOU_NAME = \"mnist_lenet5_moudle\"\n",
    "DATA_PATH = \"../dataset/Mnist/MNIST/raw\"\n",
    "\n",
    "def backward(input_x):\n",
    "    # define the input/output data type & form\n",
    "    in_x = tf.placeholder(tf.float32, [\n",
    "        BATCH_SIZE, \n",
    "        IMAGE_SIZE, \n",
    "        IMAGE_SIZE, \n",
    "        NUM_CHANNELS\n",
    "    ])\n",
    "    out_y = tf.placeholder(tf.float32,[\n",
    "        None, \n",
    "        NUM_CLASSES\n",
    "    ])\n",
    "\n",
    "    # get the prediction from forward function, now we are training the network, so dropout should be done\n",
    "    prediction = forward(in_x, train=True, regularizer=REGULARIZER)\n",
    "    \n",
    "    # init the global counter, loss, learning rate, mov_avg_decay and optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction, labels=tf.argmax(out_y, 1))\n",
    "    loss = tf.reduce_mean(cross_ent) + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate=LEARN_RATE_BASE, \n",
    "        global_step=global_step, \n",
    "        decay_steps=input_x.train.num_examples / BATCH_SIZE, \n",
    "        decay_rate=LEARN_RATE_DECAY, \n",
    "        staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss=loss, global_step=global_step)\n",
    "    exp_mov_avg = tf.train.ExponentialMovingAverage(decay=MOV_AVG_DECAY, num_updates=global_step)\n",
    "    exp_mov_avg_op = exp_mov_avg.apply(tf.trainable_variables())        # apply to all trainable variables\n",
    "\n",
    "    with tf.control_dependencies([optimizer, exp_mov_avg_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # save the moudle\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # call the session and activate the functions defined above\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # restore the moudle if we already trained one and want to continue training\n",
    "        ckpt = tf.train.get_checkpoint_state(MOU_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        # do the iterations\n",
    "        for epoch in range(EPOCH):\n",
    "            samples, labels = input_x.train.next_batch(BATCH_SIZE)\n",
    "            sample_reshaped = np.reshape(samples, (\n",
    "                BATCH_SIZE, \n",
    "                IMAGE_SIZE, \n",
    "                IMAGE_SIZE, \n",
    "                NUM_CHANNELS\n",
    "            )) \n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={in_x:sample_reshaped, out_y:labels})\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"After %d training steps(s),loss on training batch is %g.\"%(step,loss_value))\n",
    "                saver.save(sess=sess, save_path=os.path.join(MOU_PATH, MOU_NAME), global_step=global_step)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist = input_data.read_data_sets(DATA_PATH, one_hot=True)\n",
    "    backward(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "import time\n",
    "import tensorflow as tf \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np \n",
    "\n",
    "# we do a round of test every 5 seconds\n",
    "TEST_INTERVAL = 5 \n",
    "TEST_ITERATION = 10 \n",
    "\n",
    "def test(input_x):\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        in_x = tf.placeholder(tf.float32, [\n",
    "            input_x.test.num_examples, \n",
    "            IMAGE_SIZE, \n",
    "            IMAGE_SIZE, \n",
    "            NUM_CHANNELS\n",
    "        ])\n",
    "        out_y = tf.placeholder(tf.float32, [\n",
    "            None, \n",
    "            NUM_CLASSES\n",
    "        ])\n",
    "        # run the forward, remember to turn off the drop out and regularization\n",
    "        predict = forward(input_x=in_x, train=False, regularizer=None)\n",
    "\n",
    "        # restore the exponential moving average\n",
    "        exp_mov_avg = tf.train.ExponentialMovingAverage(MOV_AVG_DECAY)\n",
    "        exp_res = exp_mov_avg.variables_to_restore()\n",
    "        saver = tf.train.Saver(exp_res)\n",
    "\n",
    "        # the accuracy should be\n",
    "        correct_num = tf.equal(tf.argmax(out_y, 1), tf.argmax(predict, 1))\n",
    "        acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "        # get the Session and run the whole network\n",
    "        for itr in range(TEST_ITERATION):\n",
    "            with tf.Session() as sess:\n",
    "                # restore the target moudle\n",
    "                ckpt = tf.train.get_checkpoint_state(MOU_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "\n",
    "                    x_reshape = np.reshape(input_x.test.images, (\n",
    "                        input_x.test.num_examples, \n",
    "                        IMAGE_SIZE,\n",
    "                        IMAGE_SIZE, \n",
    "                        NUM_CHANNELS\n",
    "                    ))\n",
    "\n",
    "                    acc_score = sess.run(acc, feed_dict={in_x:x_reshape, out_y:input_x.test.labels})\n",
    "                    print(\"After %s training step(s),test accuracy = %g\"%(global_step,accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_x = input_data.read_data_sets(DATA_PATH, one_hot=True)\n",
    "    test(input_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ==To be continued===>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda9bbbebaac2b349feb322f0d71814a04b",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}