{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitenvtensorflowcondaa52f4058b2084bcda0e188f628d3ade3",
   "display_name": "Python 3.6.8 64-bit ('envTensorFlow': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# **Note on how to use pandas**\n",
    "\n",
    "### Import the libs \n",
    "- use the re lib to check if some strings are in their proper format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "import time \n",
    "\n",
    "df_health_file = \"[medical_data_path]\"\n",
    "df_educat_file = \"[education_data_path]\"\n",
    "df_target_file = \"[target_file_path]\"\n",
    "\n",
    "df_hel = pd.read_csv(df_health_file)"
   ]
  },
  {
   "source": [
    "### Basic functions\n",
    "- I feel that the 'cloumns' and 'dtypes' function are extreamly useful "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       age_at_consultation     postcode        height        weight  \\\ncount         20000.000000  15885.00000  20000.000000  20000.000000   \nmean             37.741550   3444.51300    174.865800     72.091350   \nstd              19.629359   1587.32648     19.509988     65.729232   \nmin               0.000000    800.00000     81.000000    -99.000000   \n25%              20.000000   2429.00000    169.000000     59.000000   \n50%              37.000000   3158.00000    178.000000     84.000000   \n75%              55.000000   4360.00000    188.000000    116.000000   \nmax              92.000000   7330.00000    199.000000    149.000000   \n\n                bmi  blood_pressure  cholesterol_level  smoking_status  \ncount  20000.000000    20000.000000       20000.000000    20000.000000  \nmean      28.957650       74.864050         170.642350        0.132800  \nstd        9.089631        4.010159          50.859129        0.339367  \nmin       11.000000       56.000000           1.000000        0.000000  \n25%       21.000000       72.000000         136.000000        0.000000  \n50%       28.000000       75.000000         171.000000        0.000000  \n75%       36.000000       78.000000         205.000000        0.000000  \nmax       54.000000       92.000000         363.000000        1.000000  \n"
     ]
    }
   ],
   "source": [
    "def showBasicDataInfo(df, caseNo):\n",
    "    if caseNo is 1:\n",
    "        # colums can be use as index\n",
    "        # you can retrieve an column by: \n",
    "        # df.column_name or df['column_name']\n",
    "        # notice the slice (20~39) and unique operations\n",
    "        print(df.columns)\n",
    "        print(df.email[20:40])\n",
    "        print(df['middle_name'].unique())\n",
    "    elif caseNo is 2:\n",
    "        # the number type of each column\n",
    "        print(df.dtypes)\n",
    "        # the row labels and column names\n",
    "        print(df.axes)\n",
    "    elif caseNo is 3:\n",
    "        # the size (how many cells) of the DataFrame\n",
    "        print(df.size)\n",
    "        # caculate row number\n",
    "        print(df.size / df.columns.size)\n",
    "    elif caseNo is 4:\n",
    "        # generate basic descriptive statistics such as count, mean, std, min, max, and quantiles (with diﬀerent statistics provided for non-numerical columns)\n",
    "        print(df.describe())\n",
    "    elif caseNo is 5:\n",
    "        # some functions to retrieve certain statistics\n",
    "        print(\"maximum: \\n\", df.max())\n",
    "        print(\"minimum: \\n\", df.min())\n",
    "        print(\"number of values: \\n\", df.count())\n",
    "        print(\"arithmetic average: \\n\", df.mean())\n",
    "        print(\"mode value: \\n\", df.mode())\n",
    "        print(\"median value: \\n\", df.median())\n",
    "        print(\"median absolute deviation value: \\n\", df.mad())\n",
    "        print(\"variance: \\n\", df.var())\n",
    "        print(\"standard deviation: \\n\", df.std())\n",
    "        print(\"skewness: \\n\", df.skew())\n",
    "        print(\"kurtosis: \\n\", df.kurt())\n",
    "        print(\"correlation between suitable attributes: \\n\", df.corr())\n",
    "        print(\"simple cross tabulation of two columns: \\n\", pd.crosstab(df.bmi, df.weight))\n",
    "    elif caseNo is 6:\n",
    "        # Read out the head/tail rows from the data\n",
    "        print(df.head(2))\n",
    "        print(df.tail(3))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Try to explore the data\n",
    "showBasicDataInfo(df_hel, 4)"
   ]
  },
  {
   "source": [
    "### Deal with null values\n",
    "- It is said that null values in pandas is stored as 'np.nan', but it will not work if u ues 'np.nan' to match the attribute values with '=='"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handelNaVars(df, caseNo):\n",
    "    if caseNo is 1:\n",
    "        # select the rows that have at least one missing value (at least one attribute [column] is na)\n",
    "        null_rows = df[df.isnull().any(axis=1)]\n",
    "        print(\"At least one missing:\\n\", null_rows.head())\n",
    "        # select rows that have missing values in all columns (all attributes [columns] are na)\n",
    "        null_all_rows = df[df.isnull().all(axis=1)]\n",
    "        print(\"Missing all values:\\n\", null_all_rows.head())\n",
    "    elif caseNo is 2:\n",
    "        # Drop records with missing values\n",
    "        no_null_df = df.dropna()\n",
    "        print(\"Compelete records:\\n\", no_null_df.head())\n",
    "        # Drop records if any columns contain a missing value\n",
    "        drop_any_missing = df.dropna(how='any')\n",
    "        print(\"Sould be same as the compete records:\\n\", drop_any_missing.head())\n",
    "        # Drop records where values in all columns are missing \n",
    "        drop_all_missing = df.dropna(how='all')\n",
    "        print(\"Only the records that compeltely null with be dropped:\\n\", drop_all_missing.head())\n",
    "        # Drop column if all the values are missing (for any only left complete attributes)\n",
    "        drop_all_mis_col = df.dropna(how='all', axis=1)\n",
    "        print(\"The completely null attribute will be dropped:\\n\", drop_all_mis_col.head())\n",
    "        # Drop records that contain less than 10 non-missing values \n",
    "        drop_threshold_mis = df.dropna(thresh=20)\n",
    "        print(\"The left records should all contain more than 20 non-missing attributes:\\n\", drop_threshold_mis.head())\n",
    "    elif caseNo is 3:\n",
    "        # Fill all na values with certain number\n",
    "        # Sometimes it is unstable to match nan value with np.nan, we'd better fill them with a certain value before matching\n",
    "        fill_df = df.fillna(-1)\n",
    "        print(\"After filling missing values:\\n\", fill_df.head())\n",
    "\n",
    "handelNaVars(df_hel, 2)"
   ]
  },
  {
   "source": [
    "### Some samples on doing normalisation with pandas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0   -1.666667\n1    0.000000\n2    0.333333\n3    3.666667\n4    1.000000\nName: RobustN_blood_pressure, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def nomraliseAttrs(df, caseNo):\n",
    "    if caseNo is 1:\n",
    "        # Compute the mean value\n",
    "        mean_val =  df['blood_pressure'].mean()\n",
    "        # Compute the standard deviation value \n",
    "        std_val = df['blood_pressure'].std()\n",
    "        # Zero-score\n",
    "        df['ZeroN_blood_pressure'] = (df['blood_pressure'] - mean_val) / std_val\n",
    "        print(df.head().ZeroN_blood_pressure)\n",
    "    elif caseNo is 2:\n",
    "        min_val = df.blood_pressure.min()\n",
    "        max_val = df.blood_pressure.max()\n",
    "        # Rescale to [0,1]\n",
    "        df['MinMaxN_blood_pressure'] = (df.blood_pressure - min_val) / (max_val - min_val)\n",
    "        print(df.head().MinMaxN_blood_pressure)\n",
    "    elif caseNo is 3:\n",
    "        median_val = df['blood_pressure'].median()\n",
    "        MAD = abs(df['blood_pressure'] - median_val).median()\n",
    "        # Robust normalisation\n",
    "        df['RobustN_blood_pressure'] = (df['blood_pressure'] - median_val) / MAD\n",
    "        print(df.head().RobustN_blood_pressure)\n",
    "    elif caseNo is 4:\n",
    "        df['Log_blood_pressure'] = np.log(df.blood_pressure[df.blood_pressure > 0])\n",
    "        print(df.head().Log_blood_pressure)\n",
    "\n",
    "nomraliseAttrs(df_hel, 3)"
   ]
  },
  {
   "source": [
    "### Use pandas to check missing matrix or correlation matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of missing/nan in each attribute:\n first_name           2.0\nmiddle_name       1955.0\nlast_name            1.0\npostcode          4115.0\nphone             7948.0\nemail             6057.0\nmarital_status    2521.0\ndtype: float64\nbmi ~ age at consultation (pearson):  0.238829280625467\n"
     ]
    }
   ],
   "source": [
    "def showMissing(df):\n",
    "    # df.size / df.columns.size gives us the number of samples in the file\n",
    "    # df.count() returns the number of rows that contains values (not nan/empty) from each attribute\n",
    "    df_missing = df.size / df.columns.size - df.count()\n",
    "    print(\"The number of missing/nan in each attribute:\\n\", df_missing[df_missing > 0])\n",
    "\n",
    "showMissing(df_hel)\n",
    "\n",
    "def pearsonCorrelation(df):\n",
    "    corr = df.corr(method = 'pearson')\n",
    "    # automatically ingore null/na values\n",
    "    print(\"bmi ~ age at consultation (pearson): \", corr['age_at_consultation']['bmi'])\n",
    "\n",
    "pearsonCorrelation(df_hel)"
   ]
  },
  {
   "source": [
    "### Do some validation check with pandas\n",
    "- of course, we may have tons of better ways to do these jobs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completeness for phone:  0.6026\n",
      "completeness for middle name:  0.90225\n",
      "validity for weight:  0.8975\n",
      "validity for email:  0.59705\n",
      "the consistency between age and birgh_date is:  0.8024401220061003\n"
     ]
    }
   ],
   "source": [
    "def attrInfoMeasurance(df):\n",
    "    # measure completeness\n",
    "    record_num = df.size / df.columns.size\n",
    "    complete_num = df.count()\n",
    "    print(\"completeness for phone: \", complete_num.phone / record_num)\n",
    "    print(\"completeness for middle name: \", complete_num.middle_name / record_num)\n",
    "    # measure validity\n",
    "    valid_weight_num = df[df.weight > 0].weight.size\n",
    "    print(\"validity for weight: \", valid_weight_num / record_num)\n",
    "    email_pat = \"(.*)@(.*)[.com,.cn,.net](.*)\"\n",
    "    valid_email_num = 0\n",
    "    # fill all the nan with invalid email names as they are indeed invalid and we do not like None objects\n",
    "    for email in df.email.fillna(\"NaN\"):\n",
    "        result = re.match(email_pat, email, flags=re.IGNORECASE|re.S)\n",
    "        if result:\n",
    "            valid_email_num += 1\n",
    "    print(\"validity for email: \", valid_email_num / record_num)\n",
    "    # caculate the consistency between age and birgh_date\n",
    "    # the loc function reture a slice of the dataframe\n",
    "    age_birth_df = df[df.age_at_consultation > 0].loc[:, ['age_at_consultation', 'birth_date']]\n",
    "    age_birth_df.birth_date = pd.to_datetime(age_birth_df.birth_date)\n",
    "    # it is a little bit tricky when dealing with time data, we will dig into this problem later in the notebook\n",
    "    # maximium born year\n",
    "    valid_age_df = 2020 - age_birth_df.age_at_consultation\n",
    "    valid_birth_list = []\n",
    "    for year in valid_age_df:\n",
    "        valid_birth_list.append(\"06/08/\" + str(year))\n",
    "    # maximium born date\n",
    "    valid_age_df = pd.Series(pd.to_datetime(valid_birth_list))\n",
    "    # the data is valid only when the result is positive\n",
    "    valid_age_date_df = (valid_age_df - age_birth_df.birth_date) + pd.to_datetime(\"30/12/2000\")\n",
    "    valid_num = valid_age_date_df[valid_age_date_df >= pd.to_datetime(\"30/12/2000\")].size\n",
    "    print(\"the consistency between age and birgh_date is: \", valid_num / age_birth_df.age_at_consultation.size)\n",
    "\n",
    "attrInfoMeasurance(df_hel)"
   ]
  },
  {
   "source": [
    "### Use pandas to do some statical analysis like distribution check\n",
    "- I actually still don't get the idea why Benford's law is useful"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "prob of each digit appears 1st in cholesterol level.\n",
      "prob of  1  is:  0.6318\n",
      "prob of  2  is:  0.2823\n",
      "prob of  3  is:  0.00775\n",
      "prob of  4  is:  0.0039\n",
      "prob of  5  is:  0.0063\n",
      "prob of  6  is:  0.00945\n",
      "prob of  7  is:  0.013\n",
      "prob of  8  is:  0.0192\n",
      "prob of  9  is:  0.0263\n",
      "\n",
      "prob of each digit appears 1st in blood pressure.\n",
      "prob of  1  is:  0.0\n",
      "prob of  2  is:  0.0\n",
      "prob of  3  is:  0.0\n",
      "prob of  4  is:  0.0\n",
      "prob of  5  is:  0.0001\n",
      "prob of  6  is:  0.0899\n",
      "prob of  7  is:  0.78705\n",
      "prob of  8  is:  0.12275\n",
      "prob of  9  is:  0.0002\n",
      "\n",
      "prob of each digit appears 1st in medicare number.\n",
      "prob of  1  is:  0.1132\n",
      "prob of  2  is:  0.10975\n",
      "prob of  3  is:  0.11405\n",
      "prob of  4  is:  0.11055\n",
      "prob of  5  is:  0.1104\n",
      "prob of  6  is:  0.11295\n",
      "prob of  7  is:  0.11135\n",
      "prob of  8  is:  0.1085\n",
      "prob of  9  is:  0.10925\n"
     ]
    }
   ],
   "source": [
    "# pick out the first digit from the content of the target attribute\n",
    "def getFistDigitsFromDF(targetDF, targetAttr):\n",
    "    dig_df = targetDF[targetAttr]\n",
    "    if dig_df.dtype == np.object:\n",
    "        for index in range(dig_df.size):\n",
    "            dig_df.at[index] = int(dig_df[index].strip()[0])\n",
    "    elif dig_df.dtype == np.int64:\n",
    "        for index in range(dig_df.size):\n",
    "            entry = dig_df[index]\n",
    "            while(entry >= 10):\n",
    "                entry = int(entry / 10)\n",
    "            dig_df.at[index] = entry\n",
    "    return dig_df\n",
    "\n",
    "# return the probability, check if it goes with the Benford's law\n",
    "def eachDigitProbFromDF(targetDF):\n",
    "    for digit in range(1, 10):\n",
    "        digit_prob = targetDF[targetDF == digit].size / targetDF.size\n",
    "        print(\"prob of \",digit,\" is: \",digit_prob)\n",
    "\n",
    "def benfordDistrubutionCheck(df):\n",
    "    # Benford’s law for the attributes (a) cholesterol level, (b) blood pressure and (c) medicare number\n",
    "    cho_dig_df = getFistDigitsFromDF(df, 'cholesterol_level')\n",
    "    print(\"\\nprob of each digit appears 1st in cholesterol level.\")\n",
    "    eachDigitProbFromDF(cho_dig_df)\n",
    "    bpr_dig_df = getFistDigitsFromDF(df, 'blood_pressure')\n",
    "    print(\"\\nprob of each digit appears 1st in blood pressure.\")\n",
    "    eachDigitProbFromDF(bpr_dig_df)\n",
    "    mdn_dig_df = getFistDigitsFromDF(df, 'medicare_number')\n",
    "    print(\"\\nprob of each digit appears 1st in medicare number.\")\n",
    "    eachDigitProbFromDF(mdn_dig_df)\n",
    "\n",
    "benfordDistrubutionCheck(df_hel)"
   ]
  },
  {
   "source": [
    "## A data wrangling sample\n",
    "- we want to merge the medical and the education dataset\n",
    "\n",
    "### Pipeline 1\n",
    "- the two files share a main key call 'ssn'\n",
    "- we merge the file base on this key\n",
    "- then, check the inconsistency between the two datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edu = pd.read_csv(df_educat_file)\n",
    "\n",
    "# first, let's see how many different ssn appear in two files\n",
    "all_ssn = pd.concat([df_hel.ssn, df_edu.ssn])\n",
    "# for duplicated records, only keep the first one\n",
    "dif_ssn = all_ssn.drop_duplicates()\n",
    "# only unique ssn will be kept\n",
    "rdn_ssn = all_ssn.drop_duplicates(keep=False)\n",
    "# get rid of all the unique ssn, thus we get the list of ssn that exist in both files\n",
    "sme_ssn = pd.concat([dif_ssn, rdu_ssn]).drop_duplicates(keep=False)\n",
    "# find out the records with ssn that only show up in their own file \n",
    "# use True ^ to do the xor work (TF/FT => T, TT/FF => F)\n",
    "edu_unq = df_edu.ssn[True ^ df_edu.ssn.isin(sme_ssn)]\n",
    "hel_unq = df_hel.ssn[True ^ df_hel.ssn.isin(sme_ssn)]\n",
    "# check what we got here\n",
    "print(\"SSN that appear in both files: \", sme_ssn.count())\n",
    "print(\"SSN occurred only in the education data set: \", edu_unq.count())\n",
    "print(\"SSN occurred only in the medical data set: \", hel_unq.count())\n",
    "\n",
    "# we use left join to merge the two data file with attribute 'ssn'\n",
    "df_mrg = pd.merge(df_hel, df_edu, how='left', on=['ssn'])\n",
    "\n",
    "# after merging, attrs with the same name will be added a '_x' or '_y' suffix, check if there exists any inconsistance\n",
    "def inconsist_x_yAttrs(df):\n",
    "    pair_list = ['first_name', 'middle_name', 'last_name', 'gender', 'birth_date',\n",
    "                 'street_address', 'suburb', 'postcode', 'state', 'phone', 'email']\n",
    "    for element in pair_list:\n",
    "        # pick out all the inconsistant rows\n",
    "        df_diff = df[df[element + '_x'] != df[element + '_y']]\n",
    "        # if the attribute is inconsistant, it should not be a nan value in either record\n",
    "        df_not_na = df_diff.fillna('NP_NAN')\n",
    "        df_not_na = df_not_na[True ^ df_not_na[element + '_x'].isin(['NP_NAN'])]\n",
    "        df_not_na = df_not_na[True ^ df_not_na[element + '_y'].isin(['NP_NAN'])]\n",
    "        print(\"The number of inconsistent records of current attribute \", element, \" is: \", df_diff.ssn.count(), \n",
    "        \" Number of value conflicts: \", df_not_na.ssn.count(), \" Number of missing: \", df_diff.ssn.count() - df_not_na.ssn.count())\n",
    "\n",
    "inconsist_x_yAttrs(df_mrg)\n",
    "# we remove the old identifiers and give it a new one\n",
    "df_mrg.drop(columns=['rec_id_x', 'rec_id_y'], inplace=True)\n",
    "df_mrg.rename(columns={'Unnamed: 0': 'rec_id'}, inplace=True)"
   ]
  },
  {
   "source": [
    "### Pipeline 2\n",
    "- for each attribute that appears in both datasets, we select one from the two as the final attribute \n",
    "- basically we trust the most recent one as it may be more accurant\n",
    "- however, the better way could be keep the value that most close to both records\n",
    "- finally, we remove all the attributes that no longer useful\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNewAttrFrom_x_yAttrs(df):\n",
    "    pair_list = ['first_name', 'middle_name', 'last_name', 'gender', 'birth_date',\n",
    "                 'street_address', 'suburb', 'postcode', 'state', 'phone', 'email']\n",
    "    for element in pair_list:\n",
    "        record_x = df[element + '_x'].fillna('NaN')\n",
    "        record_y = df[element + '_y'].fillna('NaN')\n",
    "        record = []\n",
    "        for rec_ptr in range(int(df.size / df.columns.size)):\n",
    "            if record_x[rec_ptr] is not 'NaN' and record_y[rec_ptr] is not 'NaN':\n",
    "                if record_x[rec_ptr] == record_y[rec_ptr]:\n",
    "                    record.append(record_x[rec_ptr])\n",
    "                # if not same, we always tend to keep the newer one as it has a higher prob to be valid\n",
    "                else:\n",
    "                    if df.consultation_timestamp[rec_ptr] > df.employment_timestamp[rec_ptr]:\n",
    "                        record.append(record_x[rec_ptr])\n",
    "                    else:\n",
    "                        record.append(record_y[rec_ptr])\n",
    "            elif record_x[rec_ptr] is 'NaN' and record_y[rec_ptr] is not 'NaN':\n",
    "                record.append(record_y[rec_ptr])\n",
    "            elif record_x[rec_ptr] is not 'NaN' and record_y[rec_ptr] is 'NaN':\n",
    "                record.append(record_x[rec_ptr])\n",
    "            # when we do not have value in either file, we set it to be np.nan\n",
    "            else:\n",
    "                record.append(np.nan)\n",
    "        # add the new column/attribute to the merge table\n",
    "        df[element] = pd.Series(record)\n",
    "    return df\n",
    "\n",
    "# remove all the reduant attributes\n",
    "def rmvReduantPairs(df):\n",
    "    pair_list = ['first_name', 'middle_name', 'last_name', 'gender', 'birth_date',\n",
    "                 'street_address', 'suburb', 'postcode', 'state', 'phone', 'email']\n",
    "    tail_list = ['_x', '_y']\n",
    "    for element in pair_list:\n",
    "        drop_col1 = element + tail_list[0]\n",
    "        drop_col2 = element + tail_list[1]\n",
    "        df.drop(columns=[drop_col1, drop_col2], inplace=True)\n",
    "    return df\n",
    "\n",
    "# first, for each attribute that appears in both files, we choose one to trust and keep\n",
    "df_mrg = createNewAttrFrom_x_yAttrs(df_mrg)\n",
    "# then we drop all the redudant attributes\n",
    "df_mrg = rmvReduantPairs(df_mrg)"
   ]
  },
  {
   "source": [
    "### Pipeline 3\n",
    "- after join, there will be some records that shares the same ssn, however, for each ssn, we just need one record to analysis\n",
    "- this time we choose records that close enough to limit changes during their time gap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the time gap between records which from two files and share the same ssn\n",
    "def timeOverlapCal(target):\n",
    "    # we only need the first 10 chars to calculate time gap \n",
    "    if (pd.Timestamp(target.consultation_timestamp.values[0][0:10]) > pd.Timestamp(target.employment_timestamp.values[0][0:10])):\n",
    "        return (pd.Timestamp(target.consultation_timestamp.values[0][0:10]) - pd.Timestamp(target.employment_timestamp.values[0][0:10]))\n",
    "    # the time gap should always be positive (as you cannot travel back)\n",
    "    else:\n",
    "        return - (pd.Timestamp(target.consultation_timestamp.values[0][0:10]) - pd.Timestamp(target.employment_timestamp.values[0][0:10]))\n",
    "\n",
    "# find the most complete record (most trustworthy compare to others)\n",
    "def compareInfoCompleteness(targetList):\n",
    "    rec_item = 0\n",
    "    min_count = 20\n",
    "    for item in targetList.rec_id:\n",
    "        target = targetList[targetList.rec_id == item]\n",
    "        count = 0\n",
    "        for value in target.values[0]:\n",
    "            if value is np.nan or value == -9999 or value == '-9999':\n",
    "                count += 1\n",
    "        if count < min_count:\n",
    "            min_count = count\n",
    "            rec_item = item\n",
    "    return targetList[targetList.rec_id == rec_item]\n",
    "\n",
    "# deal with the inconsistance of the ssn\n",
    "def solveSSNInconsistance(df):\n",
    "    # find all the ssn that appear more than once\n",
    "    red_ssn = df.ssn[True ^ df.ssn.isin(df.ssn.drop_duplicates(keep=False))]\n",
    "    red_ssn.drop_duplicates(inplace=True)\n",
    "    # we want to minimize the time gap between the merged two records\n",
    "    keep_grp = pd.DataFrame(data=None, columns=df.columns)\n",
    "    for key in red_ssn:\n",
    "        # select out all the records share the same ssn\n",
    "        ssn_grp = df[df.ssn == key]\n",
    "        time_overlap = []\n",
    "        rec_id_grp = []\n",
    "        # we use the new assigned identifier to iterate throught the list\n",
    "        for item in ssn_grp.rec_id:\n",
    "            target = ssn_grp[ssn_grp.rec_id == item]\n",
    "            rec_id_grp.append(item)\n",
    "            time_overlap.append(timeOverlapCal(target))\n",
    "        best_key = np.argmin(time_overlap)\n",
    "        if best_key is not 0:\n",
    "            apd = ssn_grp[ssn_grp.rec_id == rec_id_grp[best_key]]\n",
    "        # if we run into the situation that the time gap is the same, we check which record is more trustworthy\n",
    "        else:\n",
    "            apd = compareInfoCompleteness(ssn_grp)\n",
    "        keep_grp = pd.concat([keep_grp, apd])\n",
    "    # get rid of all the records that have ssn appear more than once in the dataset\n",
    "    df.drop_duplicates(subset=['ssn'], keep=False, inplace=True)\n",
    "    # concate the records that we want to keep\n",
    "    df = pd.concat([df, keep_grp])\n",
    "    return df\n",
    "\n",
    "# now deal with the ssn problem\n",
    "df_mrg = solveSSNInconsistance(df_mrg)"
   ]
  },
  {
   "source": [
    "### Pipeline 3\n",
    "- this time I try to profill some attributes, and I choose salary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing matrix of the current dataset, let's see what we can profill here\n",
    "df_missing = df_mrg.size / df_mrg.columns.size - df_mrg.count()\n",
    "print(\"Currently missing matrix:\\n\", df_missing[df_missing > 0])\n",
    "\n",
    "# maybe we can fill up the missings in the salary attribute with the average salary of that job\n",
    "def getMeanSalaryFilled(df):\n",
    "    occu_lis = df.occupation.drop_duplicates()\n",
    "    df['salary_temp'] = df.salary\n",
    "    # turn all the -9999 to be empty/np.nan, thus they will not take part in the average caculation\n",
    "    df.loc[df.salary_temp == -9999, 'salary'] = np.nan\n",
    "    occu_sa = []\n",
    "    occu_job = []\n",
    "    for occupation in occu_lis:\n",
    "        df_job_sa = df[df.occupation == occupation]\n",
    "        # get the average salary and the corresponding job\n",
    "        occu_sa.append(df_job_sa.salary.mean())\n",
    "        occu_job.append(occupation)\n",
    "    df.drop(columns=['salary_temp'], inplace=True)\n",
    "    # turn our results into a dataframe\n",
    "    occu_dict = {'occupation':pd.Series(occu_job), 'mean_salary':pd.Series(occu_sa)}\n",
    "    occu_df = pd.DataFrame(occu_dict)\n",
    "    df.salary.fillna(-1, inplace=True)\n",
    "    for job in occu_df.occupation:\n",
    "        # get the correspond salary to the job\n",
    "        salary = occu_df.mean_salary[occu_df.occupation == job].values.tolist()\n",
    "        # this means the value of the job attribute is np.nan\n",
    "        if len(salary) is 0:\n",
    "            continue\n",
    "        df.loc[(df.occupation == job) & (df.salary == -1), 'salary'] = salary[0]\n",
    "    # reset the value into np.nan (not provided)\n",
    "    df.loc[df.salary == -1, 'salary'] = np.nan\n",
    "    return df\n",
    "\n",
    "df_mrg = getMeanSalaryFilled(df_mrg)"
   ]
  },
  {
   "source": [
    "### Pipeline 4\n",
    "- this time I want to see if there are some values that are invalid according to their logic, format and our common sense"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's check the validation of some attributes in the data\n",
    "def printValidationStatus(df):\n",
    "    # have filled the salary but job not provided (they do not say they have no job, so maybe 0 just means they are not willing to provide their salary info)\n",
    "    df_salary = df[['rec_id','occupation','salary']]\n",
    "    df_salary.occupation.fillna(-1, inplace=True)\n",
    "    df_salary = df_salary.salary[(df_salary.occupation == -1) & (df_salary.salary == 0)]\n",
    "    print(\"The number of records that have np.nan job and 0 salary: \", df_salary.count())\n",
    "    # check the format of the Emails\n",
    "    count = 0\n",
    "    for email in df.email:\n",
    "        if email is not np.nan and not checkEmailValidation(email):\n",
    "            count += 1\n",
    "    print(\"Not valid email number: \", count)\n",
    "    # check the format of the phone numbers\n",
    "    count = 0\n",
    "    for phone in df.phone:\n",
    "        if phone is not np.nan and not checkPhoneValidation(phone.replace(' ','')):\n",
    "            count += 1\n",
    "    print(\"Not valid phone number: \", count)\n",
    "    # oops, use -9999 as missing may not be a good idea\n",
    "    df_temp = pd.read_csv(targetFilename3)\n",
    "    df_temp = df_temp[df_temp.salary == -9999]\n",
    "    print(\"The number of -9999 salary before imputed (should be np.nan): \", df_temp.size / df_temp.columns.size)\n",
    "    # is the current_age attribute updated ever year (set it to be 2020)\n",
    "    current_time = pd.Timestamp('2020-01-01')\n",
    "    df['current_age_2020'] = current_time.year - pd.to_datetime(df.birth_date).dt.year\n",
    "    df_mis_cur_age = df[df.current_age_2020 != df.current_age]\n",
    "    print(\"Now 2020, the number of mismatches in age is: \", df_mis_cur_age.size / df_mis_cur_age.columns.size)\n",
    "    df.drop(columns=['current_age_2020'], inplace=True)\n",
    "    # the invalid input in consultation time (that string cannot be turned into timestap)\n",
    "    count = 0\n",
    "    for date_time in df.consultation_timestamp:\n",
    "        try:\n",
    "            pd.Timestamp(date_time)\n",
    "        except:\n",
    "            count +=1\n",
    "    print(\"number of invalid timestamps in consulting time: \", count)\n",
    "    # check employment time\n",
    "    count = 0\n",
    "    for date_time in df.employment_timestamp:\n",
    "        try:\n",
    "            pd.Timestamp(date_time)\n",
    "        except:\n",
    "            count +=1\n",
    "    print(\"number of invalid timestamps in employment time: \", count)\n",
    "    # are all consult time match with their the consult age\n",
    "    time_lis = []\n",
    "    for date_time in df.consultation_timestamp:\n",
    "        time_lis.append(pd.Timestamp(date_time[:10]).year)\n",
    "    df['consult_age'] = pd.Series(time_lis) - pd.to_datetime(df.birth_date).dt.year\n",
    "    df_mis_cur_age = df[df.consult_age != df.age_at_consultation]\n",
    "    print(\"The number of mismaches in consulting time and age: \", df_mis_cur_age.size / df_mis_cur_age.columns.size)\n",
    "    # is there anyone shares the same email address\n",
    "    df_email = df.email.drop_duplicates(keep=False)\n",
    "    df_same_email = df.email[True ^ df.email.isin(df_email)]\n",
    "    # by using count, np.nan will not be considered\n",
    "    print(\"Duplicate email accounts: \", df_same_email.count()) \n",
    "    # weight appearantly should be a positive figure\n",
    "    df_weight = df.weight[df.weight < 0]\n",
    "    print(\"Number of weights below 0: \", df_weight.count())\n",
    "    # we know how the bmi is calculated, so...\n",
    "    df_bmi = df[['weight','height','bmi']]\n",
    "    df_qes = df_bmi[abs((df_bmi.weight / (df_bmi.height * df_bmi.height)) * 10000 - df_bmi.bmi) > 1]\n",
    "    print(\"Number of wired bmi values:\\n\", df_qes.count())\n",
    "\n",
    "def checkEmailValidation(email):\n",
    "    email_pat = \"(.*)@(.*)[.com,.cn,.net](.*)\"\n",
    "    result = re.match(email_pat, email, flags=re.IGNORECASE | re.S)\n",
    "    return result\n",
    "\n",
    "def checkPhoneValidation(phone):\n",
    "    phone_pat = \"^0[0-9]{9}$\"\n",
    "    result = re.match(phone_pat, phone, flags=re.IGNORECASE | re.S)\n",
    "    return result\n",
    "\n",
    "printValidationStatus(df_mrg)"
   ]
  },
  {
   "source": [
    "### Pipeline 5\n",
    "- now is the time to try to fix these problems, turn the invalid values into resonable values or np.nan"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base on the information we got, some invalid values can be fixed\n",
    "def fixPartInvalidData(df):\n",
    "    # we only have 0~23 hour in a day, thus 24 should be rewrite into 00\n",
    "    for date_time in df.consultation_timestamp:\n",
    "        try:\n",
    "            pd.Timestamp(date_time)\n",
    "        except:\n",
    "            df.consultation_timestamp[df.consultation_timestamp == date_time] = date_time[:11] + '00' + date_time[13:]\n",
    "    count = 0\n",
    "    # now there should not be any exception exists\n",
    "    for date_time in df.consultation_timestamp:\n",
    "        try:\n",
    "            pd.Timestamp(date_time)\n",
    "        except:\n",
    "            count += 1\n",
    "    print(\"number of invalid timestamps in consulting time: \", count)\n",
    "    # same as above\n",
    "    for date_time in df.employment_timestamp:\n",
    "        try:\n",
    "            pd.Timestamp(date_time)\n",
    "        except:\n",
    "            df.employment_timestamp[df.employment_timestamp == date_time] = date_time[:11] + '00' + date_time[13:]\n",
    "    count = 0\n",
    "    for date_time in df.employment_timestamp:\n",
    "        try:\n",
    "            pd.Timestamp(date_time)\n",
    "        except:\n",
    "            count += 1\n",
    "    print(\"number of invalid timestamps in employment time: \", count)\n",
    "    # update the current age\n",
    "    current_time = pd.Timestamp('2020-01-01')\n",
    "    age_2020 = current_time.year - pd.to_datetime(df.birth_date).dt.year\n",
    "    df.current_age = pd.Series(age_2020)\n",
    "    # also, the consult age can be fixed with the consult time\n",
    "    cons_age = pd.to_datetime(df.consultation_timestamp).dt.year - pd.to_datetime(df.birth_date).dt.year\n",
    "    df.age_at_consultation = pd.Series(cons_age)\n",
    "    # fix the invalid weight into np.nan\n",
    "    df_weight = df.weight[df.weight < 0]\n",
    "    df.loc[df.weight == -99, 'weight'] = np.nan\n",
    "    return df\n",
    "\n",
    "df_mrg = fixPartInvalidData(df_mrg)"
   ]
  },
  {
   "source": [
    "### Pipeline 6\n",
    "- we can try to add some features to help with the comming works like data mining\n",
    "- there are many attributes we can choose, for example, in NLP tasks, we can tokens from chars, words to tri-gram, word pieces, pharses generated from dependency parsing, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may choose to add some attributes to the data, for example, SLK encoding or the Soundex encoding of certain attributes\n",
    "def SLKGeneration(df):\n",
    "    df_SLK = df[['first_name', 'last_name', 'birth_date', 'gender']]\n",
    "    fam_name_attr_ind, giv_name_attr_ind, dob_attr_ind, gender_attr_ind = 1, 0, 2, 3\n",
    "    SLK_lis = []\n",
    "    for rec_values in df_SLK.values:\n",
    "        rec_values[fam_name_attr_ind] = str(rec_values[fam_name_attr_ind])\n",
    "        rec_values[giv_name_attr_ind] = str(rec_values[giv_name_attr_ind])\n",
    "        rec_values[dob_attr_ind] = str(rec_values[dob_attr_ind])\n",
    "        rec_values[gender_attr_ind] = str(rec_values[gender_attr_ind])\n",
    "        SLK_key = ''\n",
    "        # Add family name info\n",
    "        fn_len = len(rec_values[fam_name_attr_ind])\n",
    "        if fn_len < 2:\n",
    "            SLK_key = '999'\n",
    "        elif fn_len < 3:\n",
    "            SLK_key = rec_values[fam_name_attr_ind][1] + '22'\n",
    "        elif fn_len < 5:\n",
    "            SLK_key = rec_values[fam_name_attr_ind][1] + rec_values[fam_name_attr_ind][2] + '2'\n",
    "        elif fn_len >= 5:\n",
    "            SLK_key = rec_values[fam_name_attr_ind][1] + rec_values[fam_name_attr_ind][2] + rec_values[fam_name_attr_ind][4]\n",
    "        SLK_key += ' '\n",
    "        # Add given name info\n",
    "        ln_len = len(rec_values[giv_name_attr_ind])\n",
    "        if ln_len < 2:\n",
    "            SLK_key += '99'\n",
    "        elif ln_len < 3:\n",
    "            SLK_key += rec_values[giv_name_attr_ind][1] + '2'\n",
    "        elif ln_len >= 3:\n",
    "            SLK_key += rec_values[giv_name_attr_ind][1] + rec_values[giv_name_attr_ind][2]\n",
    "        SLK_key += ' '\n",
    "        # Add date of birth info\n",
    "        date_list = rec_values[dob_attr_ind].split('/')\n",
    "        for time in date_list:\n",
    "            SLK_key += time\n",
    "        SLK_key += ' '\n",
    "        # Add gender info\n",
    "        if rec_values[gender_attr_ind] is '':\n",
    "            SLK_key += '9'\n",
    "        elif rec_values[gender_attr_ind] is 'm' or 'male':\n",
    "            SLK_key += '1'\n",
    "        elif rec_values[gender_attr_ind] is 'f' or 'female':\n",
    "            SLK_key += '2'\n",
    "        else:\n",
    "            SLK_key += '9'\n",
    "        SLK_lis.append(SLK_key)\n",
    "    df['SLK_key'] = pd.Series(SLK_lis)\n",
    "    return df\n",
    "\n",
    "\n",
    "def soundexName(df):\n",
    "    soundex = {'rmv': ['a', 'e', 'i', 'o', 'u', 'y', 'h', 'w'],\n",
    "        '1': ['b', 'f', 'p', 'v'],\n",
    "        '2': ['c', 'g', 'j', 'k', 'q', 's', 'x', 'z'],\n",
    "        '3': ['d', 't'],\n",
    "        '4': ['l'],\n",
    "        '5': ['m', 'n'],\n",
    "        '6': ['r']} \n",
    "    df_fn_sdx = []\n",
    "    df_ln_sdx = []\n",
    "    # first name\n",
    "    for attr_val in df.first_name:\n",
    "        attr_val = str(attr_val)\n",
    "        soudex_attr = ''\n",
    "        if attr_val is '':\n",
    "            soudex_attr = '0000'\n",
    "        else:\n",
    "            soudex_attr = attr_val[0]\n",
    "            for idx in range(1, len(attr_val)):\n",
    "                if attr_val[idx] in soundex.get('1'):\n",
    "                    soudex_attr += '1'\n",
    "                elif attr_val[idx] in soundex.get('2'):\n",
    "                    soudex_attr += '2'\n",
    "                elif attr_val[idx] in soundex.get('3'):\n",
    "                    soudex_attr += '3'\n",
    "                elif attr_val[idx] in soundex.get('4'):\n",
    "                    soudex_attr += '4'\n",
    "                elif attr_val[idx] in soundex.get('5'):\n",
    "                    soudex_attr += '5'\n",
    "                elif attr_val[idx] in soundex.get('6'):\n",
    "                    soudex_attr += '6'\n",
    "                if len(soudex_attr) is 4:\n",
    "                    break\n",
    "            if len(soudex_attr) is not 4:\n",
    "                num_0 = 4 - len(soudex_attr)\n",
    "                soudex_attr += \"0\" * num_0\n",
    "        df_fn_sdx.append(soudex_attr)\n",
    "    # last name\n",
    "    for attr_val in df.last_name:\n",
    "        attr_val = str(attr_val)\n",
    "        soudex_attr = ''\n",
    "        if attr_val is '':\n",
    "            soudex_attr = '0000'\n",
    "        else:\n",
    "            soudex_attr = attr_val[0]\n",
    "            for idx in range(1, len(attr_val)):\n",
    "                if attr_val[idx] in soundex.get('1'):\n",
    "                    soudex_attr += '1'\n",
    "                elif attr_val[idx] in soundex.get('2'):\n",
    "                    soudex_attr += '2'\n",
    "                elif attr_val[idx] in soundex.get('3'):\n",
    "                    soudex_attr += '3'\n",
    "                elif attr_val[idx] in soundex.get('4'):\n",
    "                    soudex_attr += '4'\n",
    "                elif attr_val[idx] in soundex.get('5'):\n",
    "                    soudex_attr += '5'\n",
    "                elif attr_val[idx] in soundex.get('6'):\n",
    "                    soudex_attr += '6'\n",
    "                if len(soudex_attr) is 4:\n",
    "                    break\n",
    "            if len(soudex_attr) is not 4:\n",
    "                num_0 = 4 - len(soudex_attr)\n",
    "                soudex_attr += \"0\" * num_0\n",
    "        df_ln_sdx.append(soudex_attr)\n",
    "    df['first_name_sdx'] = pd.Series(df_fn_sdx)\n",
    "    df['last_name_sdx'] = pd.Series(df_ln_sdx)\n",
    "    return df\n",
    "\n",
    "df_mrg = soundexName(df_mrg)\n",
    "df_mrg = SLKGeneration(df_mrg)"
   ]
  },
  {
   "source": [
    "### Pipeline final\n",
    "- finally we write our results out\n",
    "- actually this can happen in any defined pipeline\n",
    "- we have to make sure that every transformation we do can be revert back\n",
    "- so most of the time we have to store a temp file before or after any pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrg.to_csv(df_target_file)"
   ]
  }
 ]
}